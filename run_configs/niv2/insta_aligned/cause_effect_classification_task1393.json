{
    "mode": "train",
    "model_id": "google/t5-xl-lm-adapt",
    "dataset_lists": [
        "Question_Answering/task1380_quarel_correct_option_generation",
        "Question_Answering/task1565_triviaqa_classification",
        "Question_Answering/task1378_quarel_correct_answer_generation",
        "Wrong_Candidate_Generation/task1379_quarel_incorrect_answer_generation",
        "Wrong_Candidate_Generation/task1381_quarel_incorrect_option_generation",
        "Program_Execution/task245_check_presence_in_set_intersection",
        "Text_Completion/task139_detoxifying-lms_classification_topicality",
        "Question_Answering/task164_mcscript_question_answering_text",
        "Question_Answering/task178_quartz_question_answering",
        "Question_Answering/task1727_wiqa_what_is_the_effect",
        "Text_Matching/task514_argument_consequence_classification",
        "Question_Answering/task165_mcscript_question_answering_commonsense",
        "Program_Execution/task124_conala_pair_averages",
        "Coherence_Classification/task069_abductivenli_classification",
        "Toxic_Language_Detection/task137_detoxifying-lms_classification_toxicity",
        "Fact_Verification/task1366_healthfact_classification",
        "Text_Categorization/task115_help_advice_classification",
        "Question_Answering/task385_socialiqa_incorrect_answer_generation",
        "Word_Semantics/task625_xlwic_true_or_false_answer_generation",
        "Commonsense_Classification/task291_semeval_2020_task4_commonsense_validation",
        "Coherence_Classification/task070_abductivenli_incorrect_classification",
        "Word_Semantics/task457_matres_conditional_classification",
        "Program_Execution/task605_find_the_longest_common_subsequence_in_two_lists",
        "Explanation/task1369_healthfact_sentence_generation",
        "Question_Answering/task1399_obqa_answer_generation",
        "Question_Answering/task1731_quartz_question_answering",
        "Text_Matching/task1288_glue_mrpc_paraphrasing",
        "Sentence_Composition/task550_discofuse_sentence_generation",
        "Program_Execution/task243_count_elements_in_set_intersection",
        "Question_Generation/task1665_trainglecopa_question_generation",
        "Wrong_Candidate_Generation/task1383_quarel_write_incorrect_answer",
        "Question_Generation/task191_hotpotqa_question_generation",
        "Question_Answering/task490_mwsc_options_generation",
        "Text_Matching/task566_circa_classification",
        "Question_Answering/task170_hotpotqa_answer_generation",
        "Sentence_Composition/task1401_obqa_sentence_generation",
        "Answer_Verification/task846_pubmedqa_classification",
        "Sentiment_Analysis/task518_emo_different_dialogue_emotions",
        "Discourse_Connective_Identification/task563_discofuse_answer_generation",
        "Fact_Verification/task403_creak_commonsense_inference",
        "Toxic_Language_Detection/task1722_civil_comments_threat_classification",
        "Question_Generation/task167_strategyqa_question_generation",
        "Information_Extraction/task456_matres_intention_classification",
        "Stance_Detection/task513_argument_stance_classification",
        "Answer_Verification/task1294_wiki_qa_answer_verification",
        "Question_Understanding/task227_clariq_classification",
        "Question_Answering/task073_commonsenseqa_answer_generation",
        "Discourse_Relation_Classification/task564_discofuse_classification",
        "Question_Generation/task1398_obqa_question_generation",
        "Explanation/task192_hotpotqa_sentence_generation",
        "Misc./task1507_boolean_temporal_reasoning",
        "Question_Generation/task1580_eqasc-perturbed_question_generation",
        "Question_Answering/task1581_eqasc-perturbed_answer_generation",
        "Dialogue_Generation/task565_circa_answer_generation",
        "Wrong_Candidate_Generation/task1400_obqa_incorrect_answer_generation",
        "Question_Answering/task1286_openbookqa_question_answering",
        "Question_Generation/task1594_yahoo_answers_topics_question_generation",
        "Question_Answering/task820_protoqa_answer_generation",
        "Explanation/task295_semeval_2020_task4_commonsense_reasoning",
        "Entity_Relation_Classification/task472_haspart_classification",
        "Toxic_Language_Detection/task322_jigsaw_classification_threat",
        "Question_Answering/task1424_mathqa_probability",
        "Question_Generation/task1657_gooaq_question_generation",
        "Question_Answering/task1564_triviaqa_answer_generation",
        "Stereotype_Detection/task320_stereoset_classification_race",
        "Question_Answering/task580_socialiqa_answer_generation",
        "Question_Generation/task1567_propara_question_generation",
        "Question_Answering/task1135_xcsr_en_commonsense_mc_classification",
        "Program_Execution/task244_count_elements_in_set_union",
        "Coherence_Classification/task1573_samsum_classification"
    ],
    "epochs": 3,
    "output_dir": "out/cause_effect_classification_task1393_top70",
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "lr": 5e-05,
    "deepspeed": "deepspeed_configs/z2_bf16_no_optim.json",
    "n_gpu": 16
}