{
    "mode": "train",
    "model_id": "google/t5-xl-lm-adapt",
    "dataset_lists": [
        "Question_Answering/task580_socialiqa_answer_generation",
        "Wrong_Candidate_Generation/task1379_quarel_incorrect_answer_generation",
        "Question_Answering/task1380_quarel_correct_option_generation",
        "Wrong_Candidate_Generation/task1381_quarel_incorrect_option_generation",
        "Question_Answering/task073_commonsenseqa_answer_generation",
        "Question_Answering/task1135_xcsr_en_commonsense_mc_classification",
        "Explanation/task295_semeval_2020_task4_commonsense_reasoning",
        "Program_Execution/task124_conala_pair_averages",
        "Question_Answering/task1286_openbookqa_question_answering",
        "Word_Relation_Classification/task1418_bless_semantic_relation_classification",
        "Text_Completion/task139_detoxifying-lms_classification_topicality",
        "Question_Answering/task1378_quarel_correct_answer_generation",
        "Stereotype_Detection/task316_crows-pairs_classification_stereotype",
        "Stereotype_Detection/task279_stereoset_classification_stereotype",
        "Program_Execution/task1189_check_char_in_string",
        "Program_Execution/task605_find_the_longest_common_subsequence_in_two_lists",
        "Program_Execution/task243_count_elements_in_set_intersection",
        "Question_Answering/task385_socialiqa_incorrect_answer_generation",
        "Question_Understanding/task046_miscellaneous_question_typing",
        "Misc./task1425_country_iso_numeric",
        "Program_Execution/task245_check_presence_in_set_intersection",
        "Misc./task1321_country_continent",
        "Misc./task1314_country_abbreviation",
        "Question_Understanding/task1328_qa_zre_relation_generation_from_question",
        "Question_Answering/task151_tomqa_find_location_easy_clean",
        "Misc./task1507_boolean_temporal_reasoning",
        "Wrong_Candidate_Generation/task631_dbpedia_14_incorrect_answer_generation",
        "Misc./task1427_country_region_in_world",
        "Misc./task1146_country_capital",
        "Text_to_Code/task130_scan_structured_text_generation_command_action_long",
        "Text_to_Code/task128_scan_structured_text_generation_command_action_short",
        "Text_to_Code/task126_scan_structured_text_generation_command_action_all",
        "Misc./task1147_country_currency",
        "Grammar_Error_Detection/task089_swap_words_verification",
        "Stereotype_Detection/task321_stereoset_classification_religion",
        "Question_Answering/task1420_mathqa_general",
        "Speaker_Identification/task855_conv_ai_2_classification",
        "Stance_Detection/task209_stancedetection_classification",
        "Stereotype_Detection/task319_stereoset_classification_profession",
        "Information_Extraction/task1510_evalution_relation_extraction",
        "Text_Completion/task140_detoxifying-lms_classification_style",
        "Discourse_Relation_Classification/task564_discofuse_classification",
        "Program_Execution/task600_find_the_longest_common_substring_in_two_strings",
        "Question_Answering/task1565_triviaqa_classification",
        "Text_Matching/task1645_medical_question_pair_dataset_text_classification",
        "Stereotype_Detection/task320_stereoset_classification_race",
        "Program_Execution/task755_find_longest_substring_and_replace_its_sorted_lowercase_version_in_both_lists",
        "Preposition_Prediction/task585_preposition_classification",
        "Question_Answering/task152_tomqa_find_location_easy_noise",
        "Stereotype_Detection/task318_stereoset_classification_gender",
        "Program_Execution/task622_replace_alphabets_in_a_list_by_their_position_in_english_alphabet",
        "Question_Answering/task1678_mathqa_answer_selection",
        "Toxic_Language_Detection/task904_hate_speech_offensive_classification",
        "Misc./task1320_country_domain_tld",
        "Misc./task1319_country_by_barcode_prefix",
        "Commonsense_Classification/task1210_atomic_classification_madeupof",
        "Commonsense_Classification/task1212_atomic_classification_hasproperty",
        "Commonsense_Classification/task1216_atomic_classification_causes",
        "Word_Semantics/task141_odd-man-out_classification_category",
        "Text_Completion/task1389_hellaswag_completion",
        "Question_Answering/task1422_mathqa_physics",
        "Commonsense_Classification/task1208_atomic_classification_xreason",
        "Story_Composition/task270_csrg_counterfactual_context_generation",
        "Program_Execution/task244_count_elements_in_set_union",
        "Story_Composition/task269_csrg_counterfactual_story_generation",
        "Text_Categorization/task1541_agnews_classification",
        "Commonsense_Classification/task1199_atomic_classification_xattr",
        "Commonsense_Classification/task291_semeval_2020_task4_commonsense_validation",
        "Question_Answering/task1423_mathqa_geometry",
        "Spam_Classification/task109_smsspamcollection_spamsmsdetection"
    ],
    "epochs": 3,
    "output_dir": "out/word_analogy_task1154_top70",
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "lr": 5e-05,
    "deepspeed": "deepspeed_configs/z2_bf16_no_optim.json",
    "n_gpu": 16
}