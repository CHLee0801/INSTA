{
    "mode": "train",
    "model_id": "google/t5-xl-lm-adapt",
    "dataset_lists": [
        "Question_Answering/task1380_quarel_correct_option_generation",
        "Wrong_Candidate_Generation/task1379_quarel_incorrect_answer_generation",
        "Question_Answering/task073_commonsenseqa_answer_generation",
        "Question_Answering/task1378_quarel_correct_answer_generation",
        "Question_Answering/task580_socialiqa_answer_generation",
        "Wrong_Candidate_Generation/task1381_quarel_incorrect_option_generation",
        "Text_Completion/task139_detoxifying-lms_classification_topicality",
        "Word_Relation_Classification/task1418_bless_semantic_relation_classification",
        "Explanation/task295_semeval_2020_task4_commonsense_reasoning",
        "Program_Execution/task243_count_elements_in_set_intersection",
        "Question_Answering/task1286_openbookqa_question_answering",
        "Question_Answering/task1135_xcsr_en_commonsense_mc_classification",
        "Speaker_Identification/task855_conv_ai_2_classification",
        "Program_Execution/task124_conala_pair_averages",
        "Question_Answering/task1565_triviaqa_classification",
        "Program_Execution/task755_find_longest_substring_and_replace_its_sorted_lowercase_version_in_both_lists",
        "Text_Completion/task140_detoxifying-lms_classification_style",
        "Program_Execution/task245_check_presence_in_set_intersection",
        "Question_Answering/task385_socialiqa_incorrect_answer_generation",
        "Text_to_Code/task130_scan_structured_text_generation_command_action_long",
        "Text_to_Code/task128_scan_structured_text_generation_command_action_short",
        "Text_to_Code/task126_scan_structured_text_generation_command_action_all",
        "Program_Execution/task1189_check_char_in_string",
        "Program_Execution/task605_find_the_longest_common_subsequence_in_two_lists",
        "Preposition_Prediction/task585_preposition_classification",
        "Question_Understanding/task046_miscellaneous_question_typing",
        "Mathematics/task085_unnatural_addsub_arithmetic",
        "Explanation/task1369_healthfact_sentence_generation",
        "Named_Entity_Recognition/task1566_propara_structured_text_generation",
        "Question_Answering/task1420_mathqa_general",
        "Commonsense_Classification/task291_semeval_2020_task4_commonsense_validation",
        "Fill_in_The_Blank/task1360_numer_sense_multiple_choice_qa_generation",
        "Toxic_Language_Detection/task137_detoxifying-lms_classification_toxicity",
        "Program_Execution/task244_count_elements_in_set_union",
        "Program_Execution/task622_replace_alphabets_in_a_list_by_their_position_in_english_alphabet",
        "Wrong_Candidate_Generation/task631_dbpedia_14_incorrect_answer_generation",
        "Stereotype_Detection/task279_stereoset_classification_stereotype",
        "Grammar_Error_Detection/task089_swap_words_verification",
        "Wrong_Candidate_Generation/task081_piqa_wrong_answer_generation",
        "Question_Generation/task023_cosmosqa_question_generation",
        "Information_Extraction/task1510_evalution_relation_extraction",
        "Stereotype_Detection/task316_crows-pairs_classification_stereotype",
        "Stance_Detection/task209_stancedetection_classification",
        "Question_Decomposition/task176_break_decompose_questions",
        "Text_Matching/task1645_medical_question_pair_dataset_text_classification",
        "Text_Categorization/task629_dbpedia_14_classification",
        "Gender_Classification/task342_winomt_classification_profession_pro",
        "Gender_Classification/task343_winomt_classification_profession_anti",
        "Commonsense_Classification/task1204_atomic_classification_hinderedby",
        "Story_Composition/task270_csrg_counterfactual_context_generation",
        "Discourse_Relation_Classification/task564_discofuse_classification",
        "Question_Answering/task1727_wiqa_what_is_the_effect",
        "Commonsense_Classification/task1216_atomic_classification_causes",
        "Question_Answering/task1423_mathqa_geometry",
        "Program_Execution/task1151_swap_max_min",
        "Misc./task1147_country_currency",
        "Sentence_Composition/task550_discofuse_sentence_generation",
        "Commonsense_Classification/task1215_atomic_classification_capableof",
        "Question_Generation/task1567_propara_question_generation",
        "Question_Answering/task151_tomqa_find_location_easy_clean",
        "Question_Understanding/task1328_qa_zre_relation_generation_from_question",
        "Question_Understanding/task834_mathdataset_classification",
        "Commonsense_Classification/task1210_atomic_classification_madeupof",
        "Program_Execution/task600_find_the_longest_common_substring_in_two_strings",
        "Pos_Tagging/task382_hybridqa_answer_generation",
        "Program_Execution/task1088_array_of_products",
        "Text_Categorization/task1541_agnews_classification",
        "Stereotype_Detection/task319_stereoset_classification_profession",
        "Program_Execution/task160_replace_letter_in_a_sentence",
        "Entity_Generation/task471_haspart_answer_generation"
    ],
    "epochs": 3,
    "output_dir": "out/word_analogy_task1156_top70",
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "lr": 5e-05,
    "deepspeed": "deepspeed_configs/z2_bf16_no_optim.json",
    "n_gpu": 16
}