{
    "mode": "train",
    "model_id": "google/t5-xl-lm-adapt",
    "dataset_lists": [
        "Question_Answering/task1565_triviaqa_classification",
        "Question_Answering/task1380_quarel_correct_option_generation",
        "Program_Execution/task245_check_presence_in_set_intersection",
        "Program_Execution/task125_conala_pair_differences",
        "Question_Answering/task666_mmmlu_answer_generation_astronomy",
        "Program_Execution/task098_conala_list_intersection",
        "Question_Answering/task490_mwsc_options_generation",
        "Question_Answering/task1378_quarel_correct_answer_generation",
        "Question_Answering/task165_mcscript_question_answering_commonsense",
        "Program_Execution/task1089_check_monotonic_array",
        "Question_Answering/task164_mcscript_question_answering_text",
        "Program_Execution/task124_conala_pair_averages",
        "Program_Execution/task243_count_elements_in_set_intersection",
        "Question_Answering/task1399_obqa_answer_generation",
        "Question_Answering/task178_quartz_question_answering",
        "Question_Answering/task693_mmmlu_answer_generation_conceptual_physics",
        "Wrong_Candidate_Generation/task1379_quarel_incorrect_answer_generation",
        "Program_Execution/task095_conala_max_absolute_value",
        "Question_Answering/task722_mmmlu_answer_generation_random_topic",
        "Question_Answering/task740_lhoestq_answer_generation_quantity",
        "Program_Execution/task1445_closest_integers",
        "Question_Answering/task073_commonsenseqa_answer_generation",
        "Program_Execution/task1446_farthest_integers",
        "Word_Semantics/task625_xlwic_true_or_false_answer_generation",
        "Question_Answering/task698_mmmlu_answer_generation_global_facts",
        "Sentence_Composition/task1401_obqa_sentence_generation",
        "Question_Answering/task309_race_answer_generation",
        "Fill_in_The_Blank/task278_stereoset_sentence_generation_antistereotype",
        "Question_Answering/task723_mmmlu_answer_generation_moral_disputes",
        "Wrong_Candidate_Generation/task1381_quarel_incorrect_option_generation",
        "Question_Answering/task741_lhoestq_answer_generation_place",
        "Question_Answering/task733_mmmlu_answer_generation_security_studies",
        "Question_Answering/task736_mmmlu_answer_generation_virology",
        "Program_Execution/task374_synthetic_pos_or_neg_calculation",
        "Question_Answering/task726_mmmlu_answer_generation_philosophy",
        "Fill_in_The_Blank/task277_stereoset_sentence_generation_stereotype",
        "Question_Generation/task1660_super_glue_question_generation",
        "Program_Execution/task506_position_of_all_alphabetical_elements_in_list",
        "Question_Generation/task1398_obqa_question_generation",
        "Question_Answering/task664_mmmlu_answer_generation_abstract_algebra",
        "Text_Categorization/task1541_agnews_classification",
        "Question_Answering/task720_mmmlu_answer_generation_marketing",
        "Question_Answering/task732_mmmlu_answer_generation_public_relations",
        "Text_to_Code/task212_logic2text_classification",
        "Stereotype_Detection/task320_stereoset_classification_race",
        "Text_Completion/task139_detoxifying-lms_classification_topicality",
        "Program_Execution/task1087_two_number_sum",
        "Program_Execution/task244_count_elements_in_set_union",
        "Question_Answering/task724_mmmlu_answer_generation_moral_scenarios",
        "Question_Answering/task1286_openbookqa_question_answering",
        "Question_Answering/task228_arc_answer_generation_easy",
        "Question_Answering/task229_arc_answer_generation_hard",
        "Question_Answering/task692_mmmlu_answer_generation_computer_security",
        "Stereotype_Detection/task319_stereoset_classification_profession",
        "Stereotype_Detection/task316_crows-pairs_classification_stereotype",
        "Question_Generation/task1602_webquestion_question_genreation",
        "Question_Generation/task1594_yahoo_answers_topics_question_generation",
        "Text_Categorization/task521_trivia_question_classification",
        "Question_Answering/task717_mmmlu_answer_generation_logical_fallacies",
        "Question_Answering/task582_naturalquestion_answer_generation",
        "Program_Execution/task091_all_elements_from_index_i_to_j",
        "Question_Answering/task718_mmmlu_answer_generation_machine_learning",
        "Question_Answering/task695_mmmlu_answer_generation_electrical_engineering",
        "Program_Execution/task606_sum_of_all_numbers_in_list_between_positions_i_and_j",
        "Question_Answering/task688_mmmlu_answer_generation_college_computer_science",
        "Program_Execution/task162_count_words_starting_with_letter",
        "Question_Answering/task887_quail_answer_generation",
        "Sentiment_Analysis/task518_emo_different_dialogue_emotions",
        "Sentiment_Analysis/task875_emotion_classification",
        "Question_Answering/task104_semeval_2019_task10_closed_vocabulary_mathematical_answer_generation"
    ],
    "epochs": 3,
    "output_dir": "out/reasoning_about_colored_objects_top70",
    "per_device_train_batch_size": 16,
    "gradient_accumulation_steps": 1,
    "lr": 5e-05,
    "deepspeed": "deepspeed_configs/z2_bf16_no_optim.json",
    "n_gpu": 16
}